import React from "react";

import Button from "direct-core/UI/Button";

var processingTime = 0;

/*
the process is like this:
1. get audio stream from mic
2. clone the stream
3. analysis on stream per 1 ms to count if user is speaking
4. every 600ms collect one block
5. in 3 , if no sound for 1000 ms , call merge block
6. block will not be merged unless last block is pushed by stop()
7. if not , this will call stop() , and with no latency , it will be called again
8. and a new block with header will be called out with new data
9.
*/
class AutoCatchAudio extends React.Component {
  chunks = []
  allClips = []
  lastVolume = 0
  noSoundFor = 0
  startTalk = false
  overAllProcessTime = 0
  state = {
    clips: []
  }

  getAudio = ref => this.audio = ref

  constructor( props ){
    super( props );
  }

  componentDidMount(){
    navigator.mediaDevices.getUserMedia({
      audio: true
    }).then( stream => {
      this.mediaRecorder = new MediaRecorder( stream )
      this.perpareStreamAnalysis( stream.clone() )
      // this let me know the sound speaker makes every 1 ms
      this.mediaRecorder.ondataavailable = this.dataProcess
      // this gives me a blob
    }).catch( err => {
      console.log( err );
    })
  }




  perpareStreamAnalysis = ( stream ) => {
    const audioCtx = new AudioContext();
    const mediaStreamSource = audioCtx.createMediaStreamSource( stream.clone() );
    var processor = audioCtx.createScriptProcessor();

  	processor.onaudioprocess = ( event ) => {
    	const buf = event.inputBuffer.getChannelData( 0 );
      const length = buf.length;
    	var sum = 0;
      var x;
      processingTime++;
      this.overAllProcessTime++;
    	// Do a root-mean-square on the samples: sum up the squares...
      for( let i = 0; i < length ; i++ ){
      	x = buf[i];
      	sum += x * x;
      }

      const rms = Math.sqrt( sum / length );
      processor.volume += rms;
    }

  	processor.volume = 0;
    // no effect

    this.meter = processor;

    mediaStreamSource.connect( this.meter );
    processor.connect( audioCtx.destination );
    setInterval( this.streamAnalysis , 10 )
  }

  /*
  called every 10ms
  */
  streamAnalysis = () => {
    if( !this.overAllProcessTime ){
      return
    }
    //console.log( this.startTalk , this.meter.volume / this.overAllProcessTime );
    if( !this.startTalk ){
      if( this.meter.volume / this.overAllProcessTime >= 0.05 ){
        this.startTalk = true;
      }
      this.overAllProcessTime = this.meter.volume = 0;
      return
    }

    if( this.meter.volume / this.overAllProcessTime <= 0.05 ){
      this.noSoundFor += 10;
      if( this.noSoundFor > 1000 ){
        if( this.startTalk ){
          this.mergeRecords();
          this.startTalk = false;
        }
      }
    } else {
      this.noSoundFor = 0;
    }
    this.overAllProcessTime = 1;
    this.meter.volume = 0;
  }

  onBlockEnd = () => {
    this.mediaRecorder.requestData();
    this.isLastBlockStop = false;
  }

  /*
  this will be called every 600ms with a start block , a data block or a end block
  when merging , it must be start-data-data-....-data-end
  so merge function must be excuted here
  and mergeRecords is just a request
  so , if requested , and last block is not a end block,
  this function will immediately call this.stopReord
  which implements the stop of the record and set a signal
  which indicates last block is triggered as stop block
  after merge , all flags are reset
  if no merge request , this function just collect every new block

  in a complete system, a merge request will be fired when user haven't
  talk for 1000 ms
  */
  dataProcess = ( ev ) => {
    this.chunks.push( ev.data )
    if( this.requestMerge ){
      if( !this.isLastBlockStop ){
        this.stopReord();
      } else {
        this.requestMerge = false;
        this.startRecord();
        this.isLastBlockStop = false;
        var blob = new Blob( this.chunks , {
          type: "audio/ogg; codecs=opus"
        });
        this.chunks = [];
        var audioURL = URL.createObjectURL( blob );
        this.allClips.push( audioURL );
        this.audio.src = audioURL;
      }
    }
  }

  startRecord = () => {
    // this switch is for UI-control
    switch( this.mediaRecorder.state ){
      case "paused":
        this.mediaRecorder.resume();
        break;
      case "inactive":
        this.mediaRecorder.start();
        break;
      default:
        return;
    }
    this.handle = setInterval( this.onBlockEnd , 600 );
  }

  stopReord = () => {
    // this line is for UI-control
    if( this.mediaRecorder.state === "inactive" ){
      return
    }
    this.mediaRecorder.stop();
    clearInterval( this.handle );
    this.isLastBlockStop = true;
  }

  mergeRecords = () => {
    /*
    when this method is called
    last chunk has benn pushed
    and next chuck will be a start chucnk
    */
    this.requestMerge = true;
    this.noSoundFor = 0;
  }

  debug = () => {
    this.setState({
      clips: this.allClips
    })
    console.log( processingTime );
  }

  render(){
    return (
      <React.Fragment>
        <Button
          onClick={this.startRecord}
          text="开始"
        />
        <Button
          onClick={this.mergeRecords}
          text="强制合成"
        />
        <Button
          onClick={this.stopReord}
          text="终止"
        />
        <Button
          onClick={this.debug}
          text="debug"
        />
        <div>
          <span>Main audio</span>
          <audio
            controls
            ref={this.getAudio}
          />
          {
            this.state.clips.map( clip => (
              <audio
                key={clip}
                controls
                src={clip}
              />
            ))
          }
        </div>
      </React.Fragment>
    )
  }
};

export default AutoCatchAudio;
